# Kafka topic sink: {{ node_id }} (DLT sink API)
# Create Kafka sink target
dp.create_sink(
    name="{{ node_id }}",
    format="kafka",
    options={
        "kafka.bootstrap.servers": "{{ bootstrap_servers }}",
        "topic": "{{ topic }}"{% if security_protocol and security_protocol != 'PLAINTEXT' %},
        "kafka.security.protocol": "{{ security_protocol }}"{% endif %}{% if sasl_mechanism %},
        "kafka.sasl.mechanism": "{{ sasl_mechanism }}"{% endif %}{% if sasl_jaas_config %},
        "kafka.sasl.jaas.config": "{{ sasl_jaas_config }}"{% endif %}{% if ssl_truststore_type %},
        "kafka.ssl.truststore.type": "{{ ssl_truststore_type }}"{% endif %}{% if ssl_truststore_location %},
        "kafka.ssl.truststore.location": "{{ ssl_truststore_location }}"{% endif %}

    }
)

# Append flow to write from upstream to Kafka
@dp.append_flow(name="{{ node_id }}_flow", target="{{ node_id }}")
def {{ node_id }}_flow():
    return spark.readStream.table("{{ source_table }}").selectExpr("{{ select_expr }}")
